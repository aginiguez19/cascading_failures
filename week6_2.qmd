---
title: "Cascading Failures"

author: "Abraham Iñiguez and Marwin Carmo"
bibliography: references.bib
csl: https://www.zotero.org/styles/apa
---


# Failures in graphs?

Well established in this class is the fact that graphs are made up of nodes (components) and edges (links). We have many real-world examples of graphs to take from: the internet, power grids, ecology, physiology, and of course psychological systems.

In these networks often times we see the interplay of processes internal and external to the system. Think of a transmission line malfunctioning in an electrical power grid. Or possibly a perturbation to a system that disrupts communication among nodes. These two examples hint at an important dynamical process termed cascading failure or avalanche (if you're feeling fancy). Although rare, they can have severe consequences. In general, a cascading failure refers to some trigger mechanism that produces further failures in the system which ultimately may lead to a complete collapse of the system [@valdez2020jcn]. Further failures occur because components vary in their ability to handle a certain load that is redistributed to them after an initial failure.

Similarly, in social systems we might think of a cascading failure being dependent on the information other people may have and subsequently their decisions. This is quite common in fads or things that are "hyped-up". We might choose to wear Levi jeans because many of our peers are wearing Levi jeans. We might choose to see one movie over another based on reviews or recommendations by friends. Economists define these scenarios as *binary decisions with externalities* [@watts2002p]. Important to note is that a personal threshold must be reached before a transition occurs in conjunction with what others have decided. Furthermore, rather than components failing it is a component transitioning from one state to another.

<!-- Similarly, in social systems we might think of a cascading failure being dependent on the information other people may have and subsequently their decisions. This is quite common in fads or things that are "hyped-up". We might choose to wear Levi jeans because many of our peers are wearing Levi jeans. We might choose to see one movie over another based on reviews or recommendations by friends. Economists define these scenarios as *binary decisions with externalities*[@watts2002p]. Important to note is that a personal threshold must be reached before a transition occurs in conjunction with what others have decided. Furthermore, rather than components failing it is a component transitioning from one state to another. -->


Cascading failures leads us to questions such as:

-   How does the structure of a network impact its resilience to failures?

-   How can we enhance a system to mitigate further cascading failures?

-   Where are the vulnerable components in a system. Are they the high-degree or low-degree nodes?

-   When do components transition states (i.e., at a well-defined threshold)?

As an illustrative example:

::: center
![](images/EnergyGrid.jpg){fig-align="center" width="90%"}
:::

A group of students are running experiments and simulations in their respective labs. Suddenly a blackout occurs! Welp, there goes their precious research. Let's explain to these poor students that a cascading failure may have occurred.

1.  Overloading or zeroing of a component in the electrical power grid. *A group of Davis bikers crash into a substation*.

2.  Power is redistributed from the failed component. There is an increase in the loadings of other components.

3.  The system is stressed out and loadings are higher than the capacity of components. Leading to a cascading failure.

4.  Blackout all around Davis.

# Recap on Graph Structures

## Interactive Display

### PA

```{r echo = FALSE, message = FALSE}
library(igraph)
library(visNetwork)
g = sample_pa(n = 20, power = 1)
V(g)$name = as.character(1:vcount(g))
nodes = data.frame(id = V(g)$name, label = V(g)$name)
edges = as_data_frame(g, what = "edges")
visNetwork(nodes, edges) %>%
  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE) %>%
  visPhysics(stabilization = TRUE) %>%
  visInteraction(dragNodes = TRUE, dragView = TRUE)
```

### ER

```{r echo = FALSE, message = FALSE}
g = sample_gnp(n = 20, p = .5)
V(g)$name = as.character(1:vcount(g))
nodes = data.frame(id = V(g)$name, label = V(g)$name)
edges = as_data_frame(g, what = "edges")
visNetwork(nodes, edges) %>%
  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE) %>%
  visPhysics(stabilization = TRUE) %>%
  visInteraction(dragNodes = TRUE, dragView = TRUE)
```

### WS

```{r echo = FALSE}
g = sample_smallworld(dim = 1, size = 20, nei = 2, p = .5)
V(g)$name = as.character(1:vcount(g))
nodes = data.frame(id = V(g)$name, label = V(g)$name)
edges = as_data_frame(g, what = "edges")
visNetwork(nodes, edges) %>%
  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE) %>%
  visPhysics(stabilization = TRUE) %>%
  visInteraction(dragNodes = TRUE, dragView = TRUE)
```


<!-- -   Focus on re-describing the Erdős–Rényi and Barabási–Albert random graphs -->
<!--     -   With Barabási–Albert, talk more in-depth about preferential attachment and the "growth" of graphs -->
<!--     -   E.g., graphs in the real-world add and incorporate vertices over time and the addition of vertices follows some known procedures -->
<!--     -   Define the terms: Preferential attachment and Assortative mixing and how they relate to the evolution of a network's structure -->

## Compare and Contrast to these idea of "controlling" a network

# The Watt's model for Cascading Failures

As aforementioned, cascading failures can be thought of as the transitioning of a component from one state to another influenced by the state of neighboring components. The Watt's model [@watts2002p] focuses on small initial shocks relative to the size of the network that cascade into large failures of the system. An interesting aspect of cascading failures is that not all shocks result in a global cascade. As we'll find out, some nodes have a higher threshold and therefore are less vulnerable. Or some nodes are simply less connected than other nodes despite being vulnerable. In short, it's difficult to predict which node will result in a cascading failure without knowing the structure of the network.

## Formal Definition of the Watt's model

Formally, the Watt's model can be described as a *simple binary decision rule with externalities*:

1.  **We have a network of components**, G = ($C$, $E$):

-   $C_i$ observes the states of $K$ other components, essentially its *degree*.
-   $C_i$ adopts state 1 if at least a threshold fraction ($\phi$) of its $K$ neighbors are state 1, otherwise it stays inactive.

2.  **Component threshold**:

-   To account for component variation, both the threshold and number of $K$ neighbors are allowed to be heterogeneous. Each component is assigned a threshold $\phi$ randomly drawn from a distribution $f(\phi)$ where $f(\phi)$ is a distribution defined on $[0,1]$, with $\int_0^1 f(\phi) \, d\phi = 1$.

3.  **Network connection**:

-   Each component is connected to $K$ neighbors with probability $P_k$ and the average number of neighbors is $\langle k \rangle = z$. Simply, $P_k$ is the degree distribution of the graph and $z$ is the *average degree.*

4.  **Modeling Dynamics of Cascades**

-   The population is initially all off (state 0) and perturbed at $t$ = 0 by a small fraction of $\phi_0 \ll 1$ that are switched to state 1.

5.  **Subsequent Steps:**

-   The system evolves at successive time steps with each component updating their state according to their threshold and the rule mentioned in $\text{step 1}$. Once a component is active in this system it will stay active for the duration of time steps.

**Important Considerations**

-   *Local dependencies* occur in that the threshold rule ensures a component's state is determined by the states of neighboring nodes.
-   *Fraction not a max*, the more signal one receives when making a choice, the less important the signal becomes. A key reason why we love In-N-Out's short menu. In other words, the threshold rule does not take into account all of a component's neighbors but a fraction.
-   *A heterogeneous system*, this one is self-explanatory.

# Coding the Watt's Model

```{r echo = TRUE, results = 'hide', message = FALSE}
#Libraries
library(ggplot2)
library(tidyr)
library(dplyr)
```

## Watt's cascade function

```{r}
cascade.Watts = function(g, thrsh.lim = c(0.1, 0.20), max.steps = 10000){
  Cascades = list()
  metrics = igraph::degree(g)
  # Identify the high, average, and low degree vertices
  max_node = which.max(metrics)
  if(min(metrics, na.rm = TRUE) == 0){
    min_node = which.min(ifelse(metrics != 0 & !is.na(metrics), metrics, 999))
  }else(min_node = which.min(ifelse(!is.na(metrics), metrics, 999)))
  avg_node = sample(which(abs(metrics - floor(mean(metrics, na.rm = TRUE))) == min(abs(metrics - floor(mean(metrics, na.rm = TRUE))), na.rm = TRUE) & !is.na(metrics)), 1)
  seeds = list(high = max_node, 
               avg = avg_node,
               low = min_node)
  # For the high, middle, and low vertices initiate a cascade
  for(name in names(seeds)) {
    target = seeds[[name]]
    if(thrsh.lim[1] > thrsh.lim[2]){
      message("First limit should be the lower limit! Flipping for you")
      thrsh.lim = c(thrsh.lim[2], thrsh.lim[1])
    }
    # Uniformly apply thresholds to all vertices between limits
    V(g)$threshold = runif(vcount(g), min = thrsh.lim[1], max = thrsh.lim[2])
    V(g)$state = 0
    V(g)$state[target] = 1
    flip.count = c(sum(V(g)$state))
    step = 0
    # Initiate Cascade
    while(step < max.steps) {
      step = step + 1
      new.state = V(g)$state
      # For each vertex, check if it failed. If so, skip
      # If not, then identify its neighbors
      # Check how many of the current vertex's neighbors have failed relative to total
      # Compare to threshold; if bigger then fail
      # Repeat
      for (v in V(g)) {
        if (V(g)$state[v] == 1) next
        neighbors.v = neighbors(g, v)
        if (length(neighbors.v) == 0) next
          active_fraction = sum(V(g)$state[neighbors.v]) / length(neighbors.v)
        if (active_fraction >= V(g)$threshold[v]) {
          new.state[v] = 1
        }
      }
      if(all(new.state == V(g)$state)) break
      V(g)$state = new.state
      flip.count = c(flip.count, sum(V(g)$state))
    }
    Cascades[[name]] = flip.count / vcount(g)
  }
  names(Cascades) = c("High", "Average", "Low")
  return(Cascades)
}
```

## Simulation function

```{r}

# Simulation Function
sim.funk = function(rep_id) {
  set.seed(3887891 + rep_id) 
  g_ba = sample_pa(n = n_nodes, power = 1, m = 1, directed = FALSE)
  g_er = sample_gnm(n = n_nodes, m = ecount(g_ba), directed = FALSE)
  g_sw = sample_smallworld(dim = 1, size = n_nodes, nei = floor(ecount(g_ba)/n_nodes), p = 0.05)
  ER.set = cascade.Watts(g_er, c(0.1, 0.2), 10000)
  BA.set = cascade.Watts(g_ba, c(0.1, 0.2), 10000)
  SW.set = cascade.Watts(g_sw, c(0.1, 0.2), 10000)
  list(
    ER_high = ER.set$High,
    ER_avg  = ER.set$Average,
    ER_low  = ER.set$Low,
    BA_high = BA.set$High,
    BA_avg  = BA.set$Average,
    BA_low  = BA.set$Low,
    SW_high = SW.set$High,
    SW_avg  = SW.set$Average,
    SW_low  = SW.set$Low
  )
}
```

## Plot

```{r}

# Initial Parameters for Simulation
n_nodes = 30

# Use function
results = sim.funk(1245)
models = c("ER","BA","SW")
max_len = 30
op = par(mfrow=c(3,1), mar=c(4,4,2,1))

for(m in models) {
  idx = grep(paste0("^", m, "_"), names(results))
  raw = results[idx]
  mat = sapply(raw, function(v) {
    L = length(v)
    if (L < max_len) {
      c(v, rep(v[L], max_len - L))
    } else {
      v[1:max_len]
    }
  })
  
  matplot(1:max_len, mat,
          type = "o", pch = 1:3,
          col = 1:3, xlab = "Step",
          ylab = "Prop. Failed",
          main = paste(m, "model"),
          xlim = c(1, max_len),
          ylim = c(0, 1))
  legend("bottomright", legend = names(results)[idx],
         col = 1:3, pch = 1:3, bty = "n")
}
par(op) 
```

## What if we change the parameters?

### N = 300 with same threshold

```{r echo = FALSE}
# Initial Parameters for Simulation
n_nodes = 300

# Simulation Function
sim.funk = function(rep_id) {
  set.seed(3887891 + rep_id) 
  g_ba = sample_pa(n = n_nodes, power = 1, m = 1, directed = FALSE)
  g_er = sample_gnm(n = n_nodes, m = ecount(g_ba), directed = FALSE)
  g_sw = sample_smallworld(dim = 1, size = n_nodes, nei = floor(ecount(g_ba)/n_nodes), p = 0.05)
  ER.set = cascade.Watts(g_er, c(0.1, 0.2), 10000)
  BA.set = cascade.Watts(g_ba, c(0.1, 0.2), 10000)
  SW.set = cascade.Watts(g_sw, c(0.1, 0.2), 10000)
  list(
    ER_high = ER.set$High,
    ER_avg  = ER.set$Average,
    ER_low  = ER.set$Low,
    BA_high = BA.set$High,
    BA_avg  = BA.set$Average,
    BA_low  = BA.set$Low,
    SW_high = SW.set$High,
    SW_avg  = SW.set$Average,
    SW_low  = SW.set$Low
  )
}



results = sim.funk(1245)
models = c("ER","BA","SW")
max_len = 30
op = par(mfrow=c(3,1), mar=c(4,4,2,1))

for(m in models) {
  idx = grep(paste0("^", m, "_"), names(results))
  raw = results[idx]
  mat = sapply(raw, function(v) {
    L = length(v)
    if (L < max_len) {
      c(v, rep(v[L], max_len - L))
    } else {
      v[1:max_len]
    }
  })
  
  matplot(1:max_len, mat,
          type = "o", pch = 1:3,
          col = 1:3, xlab = "Step",
          ylab = "Prop. Failed",
          main = paste(m, "model"),
          xlim = c(1, max_len),
          ylim = c(0, 1))
  legend("bottomright", legend = names(results)[idx],
         col = 1:3, pch = 1:3, bty = "n")
}
par(op) 

```

### N = 30, with threshold between .05 and .9

```{r echo = FALSE}
# Initial Parameters for Simulation
n_nodes = 30


# Simulation Function
sim.funk = function(rep_id) {
  set.seed(3887891 + rep_id) 
  g_ba = sample_pa(n = n_nodes, power = 1, m = 1, directed = FALSE)
  g_er = sample_gnm(n = n_nodes, m = ecount(g_ba), directed = FALSE)
  g_sw = sample_smallworld(dim = 1, size = n_nodes, nei = floor(ecount(g_ba)/n_nodes), p = 0.05)
  ER.set = cascade.Watts(g_er, c(0.05, 0.9), 10000)
  BA.set = cascade.Watts(g_ba, c(0.05, 0.9), 10000)
  SW.set = cascade.Watts(g_sw, c(0.05, 0.9), 10000)
  list(
    ER_high = ER.set$High,
    ER_avg  = ER.set$Average,
    ER_low  = ER.set$Low,
    BA_high = BA.set$High,
    BA_avg  = BA.set$Average,
    BA_low  = BA.set$Low,
    SW_high = SW.set$High,
    SW_avg  = SW.set$Average,
    SW_low  = SW.set$Low
  )
}



results = sim.funk(1245)
models = c("ER","BA","SW")
max_len = 30
op = par(mfrow=c(3,1), mar=c(4,4,2,1))

for(m in models) {
  idx = grep(paste0("^", m, "_"), names(results))
  raw = results[idx]
  mat = sapply(raw, function(v) {
    L = length(v)
    if (L < max_len) {
      c(v, rep(v[L], max_len - L))
    } else {
      v[1:max_len]
    }
  })
  
  matplot(1:max_len, mat,
          type = "o", pch = 1:3,
          col = 1:3, xlab = "Step",
          ylab = "Prop. Failed",
          main = paste(m, "model"),
          xlim = c(1, max_len),
          ylim = c(0, 1))
  legend("bottomright", legend = names(results)[idx],
         col = 1:3, pch = 1:3, bty = "n")
}
par(op) 

```

# The CASCADE model for Cascading Failures

The CASCADE is a probabilistic model for understanding the propagation of cascading failures [@dobson2007caijons]. It was developed to capture the weakening of a system as a cascade proceeds, particularly in the context of blackout of power transmitting systems, but made sufficiently simple and general that it could be applied to cascading failure in various large, interconnected infrastructures. CASCADE focuses on components failing when their load exceeds a threshold, how an initial disturbance loads the system, and how the failure of some components transfers additional load to others.

## Formal definition of the CASCADE model:

The model considers a system with $N$ components, each with a random initial load.

1.  **Initial load** ($L_j$):

-   Each component $j$ (for $j = 1, 2, ..., N$) has an initial load $L_j$.
-   The minimum value of a given load is $L^{\text{min}}$ and the maximum load is $L^{\text{max}}$.
-   $L_{j}$ is distributed uniformly between $[L^{\text{min}}, L^{\text{max}}]$.
-   The initial loads $L_{1},L_{2},...,L_{N}$ are independent of each other.

2.  **Failure threshold** ($L^\text{fail}$):

-   Components fail if their total load exceeds a certain failure threshold, $L^\text{fail}$

3.  **Load transfer** ($P$):

-   When a component fails, a fixed amount of additional load, $P$, is transferred to each of the other remaining components in the system.

4.  **Initial disturbance** ($D$):

-   The cascade is initiated by an initial disturbance that adds an amount of load $D$ to each component.

5.  **Normalized initial load** ($\ell_{j}$):

-   The normalized initial load for the $j^{th}$ vertex is given by $\ell_{j}$ where:

$$
  \ell_{j} = \frac{L_{j} - L^{\text{min}}}{L^{\text{max}} - L^{\text{min}}}
$$

-   This normalization maps the load $L_j$ to a value $\ell_{j}$ that is uniformly distributed on the interval $[0,1]$.

6.  **Normalized parameters**:

-   The load transfer $P$ and initial disturbance $D$ are also normalized. Let $p$ be the normalized load increment and $d$ be the normalized initial disturbance:

$$
\begin{aligned}
  p = & \frac{P}{L^{\text{max}}-L^{\text{min}}} \\
  d = & \frac{D + L^{\text{max}} - L^{\text{fail}}}{L^{\text{max}}-L^{\text{min}}}
\end{aligned}
$$

7.  **Cascading process**:

-   After the initial disturbance $D$, components whose total load $(L_j+D)$ exceeds $L^{\text{fail}}$ will fail.
-   The failure of these components then distributes an additional load $P$ (or $p$ in normalized terms) to other components, potentially causing further failures. This process continues iteratively, defining the cascade.

<!-- # Compare and Contrast Watts' Cascades from the CASCADE Model -->

<!-- > The Watt's description is more simplistic, vertices have no thresholds internal to themselves but are influenced entirely by neighbors. In the CASCADE model, vertices have resilience and an initial load such that they can exhibit more nuanced dynamics in how they fail -->

# Watt's Cascades vs. CASCADE Model

A brief distinction between the two models is that the Watt's description is more simplistic: vertices have no thresholds internal to themselves but are influenced entirely by neighbors. In the CASCADE model, vertices have resilience and an initial load such that they can exhibit more nuanced dynamics in how they fail. Let's break down their differences.

### Thresholds and Failure

-   **Watt's model**: Failure is determined by the fraction of a node's *neighbors* that have already failed. A node $i$ with degree $k_i$ and threshold $\phi_i$ fails if at least a threshold fraction $\phi$ out of its $k$ neighbors are in state 1 (fail state), otherwise it adopts state 0. The threshold $\phi_i$ represents the node's susceptibility to peer influence or stress from neighbors. Vertices do not have an internal "load" capacity in the same way as the CASCADE model; their failure is purely a response to the state of their local neighborhood.

-   **CASCADE**: Failure is determined by a node's internal load exceeding its predefined capacity ($L^{\text{fail}}$). Nodes have an initial load $L_j$, and this load increases due to an initial system-wide disturbance $D$ and subsequent load transfers $P$ from other failed components. The threshold is an absolute load limit, not directly dependent on the number or state of its neighbors in the topological sense.

### Influence mechanism

-   **Watt's model**: Influence is local and topological. A node is influenced directly and only by the state (failed or not) of its immediate neighbors in the graph. What matters is the structure of the network (who is connected to whom).

-   **CASCADE**: Influence is through load redistribution. When a node fails, it transfers a fixed load $P$ to other components. While the underlying network structure can be implicitly present in how loads might realistically be distributed, the primary mechanism is the global or semi-global impact of load addition.

### Vertex properties

-   **Watt's model**: Vertices are primarily characterized by their degree $k$ and their individual threshold $\phi$. There isn't an explicit concept of "resilience" beyond the threshold itself.


- **CASCADE**: Vertices are characterized by their initial load $L_j$ and a common failure load $L^{\text{fail}}$. The "resilience" of a node is the difference between its current load and $L^{\text{fail}}$. Nodes can have different initial loads, making some inherently closer to failure than others even before the cascade begins. This implies that a node with a high initial load is more vulnerable.

### Complexity of Dynamics

- **Watt's model**: The dynamics are driven by changes in neighbor states. The "vulnerable" nodes (low threshold relative to degree) play a crucial role in initiating widespread cascades. 
  - In the *low connectivity* regime, the network is sparse, and cascade propagation is mainly limited by connectivity. At the critical point (the system transitions from safe to vulnerable), cascade sizes follow a power law.
  - In the *high connectivity* regime the network is dense. The propagation of cascades is limited by the stability of individual nodes. Most nodes have many neighbors, making them less susceptible to single-node influence. This results in a bimodal distribution of cascade sizes: many small cascades that die out quickly and, rarely, extremely large global cascades.

- **CASCADE**: The system dynamics are driven by accumulating load, with overall stress being a key factor. This stress depends on how close the initial loads are to the failure threshold ($L^{\text{fail}}$) and the magnitudes of $D$ and $P$. It examines the probability distribution of failed components and how this changes as system parameters, such as overall loading or load transfer ($P$), approach a critical point.


In summary, Watts' model is more focused on the contagion of states through local, fractional influence within a network, where nodes don't have intrinsic load capacities but rather susceptibility thresholds. The CASCADE model, conversely, emphasizes the accumulation of load on components that have defined capacities, where failure is a consequence of exceeding this capacity, leading to further load redistribution.

> Given these differences, which cascading failure model would be more appropriate to your research area?

## Can We Know the Thresholds for Vertices?

In the CASCADE model, the threshold is essentially defined by a node's capacity and its load. In Watts' model, the threshold ($\phi$) for a node determines the fraction of its neighbors that must be "on" for it to switch "on."

In **engineered/physical systems**, the thresholds can often be estimated or are known from design specifications. For example, a power line has a maximum current capacity, a router has a maximum data throughput, and a bridge has a maximum weight limit. These thresholds are based on physical properties and engineering design tolerances. However, even in these systems, thresholds can be dynamic, affected by environmental conditions (e.g., temperature affecting power line sag and capacity), aging, maintenance levels, or even "hidden failures" where a component's actual capacity is less than assumed.

In **social/economical/biological systems** a threshold could relate to the point at which an individual or entity changes their behavior, opinion, or state due to influence from others or accumulated stress. In Watts' model, this is the fraction of neighbors adopting a behavior that triggers an individual to also adopt it. For example, in decision making people could assume others have better information and follow their actions. It could also be the desire to align with the perceived majority or norm, or to be influenced by trusted sources. A biological "failure" can be illustrated by a PhD student who becomes overwhelmed and unable to progress with their research. In this scenario, the threshold could be influenced by factors such as the volume of TA responsibilities, the complexity of the research, and the level of support received from their advisor.

> What is a "threshold" for psychological vertices? Is it reasonable to assume psychological symptoms *have* thresholds for activation influenced by connected symptoms?

<details>

<summary>An example of insomnia development.</summary>

Let's consider insomnia as a complex system where various psychological and behavioral factors interact. We can conceptualize the transition into a state of chronic insomnia, or the worsening of its symptoms, as a form of cascading failure within an individual's "sleep regulation network." Let's build our example using the popular **3P model** [@spielmanBehavioralPerspectiveInsomnia1987]:

-   **Predisposing Factors**: These are like the initial conditions or inherent vulnerabilities of certain nodes in our network. Think of genetic propensity, a tendency towards ruminative thoughts, or a generally anxious disposition. These factors don't cause insomnia on their own but lower the "failure threshold" of certain psychological states. For example, a "ruminative thought" node might have a naturally lower threshold to become "active" (i.e., start interfering with sleep).

-   **Precipitating Factors**: These are the initial "shocks" or "failures" in the system. A stressful life event, a temporary illness, or a change in routine could be a precipitating factor that "activates" or "causes the failure" of an initial set of nodes – perhaps leading to a few nights of poor sleep. In our cascade model, this is the initial $\phi_0$ of "failed" nodes.

-   **Perpetuating Factors**: These are crucial for the "cascade." These are behaviors and beliefs adopted to cope with initial sleep disturbance but end up reinforcing and exacerbating it.

-   Take, for example, the node "Excessive Time in Bed": An individual experiences a few nights of poor sleep (precipitating factor). To compensate, they start spending excessive time in bed (a perpetuating behavior). This "Excessive Time in Bed" node now becomes "active."

-   Neighboring Nodes & Thresholds:

  -   This "Excessive Time in Bed" node influences other nodes, like "Sleep Fragmentation" and "Weakened Sleep-Wake Cycle."
  -   The "Sleep Fragmentation" node might have a threshold: if "Excessive Time in Bed" is active AND, say, "Anxiety about Sleep" (another node, perhaps activated by the precipitating factor) is also active, its threshold is met, and it "fails" (sleep becomes more fragmented).
  -   The "Weakened Sleep-Wake Cycle" node might fail if "Excessive Time in Bed" stays active for a certain duration or intensity.

</details>

# Weave in Graph Structures

## Contrasting Erdős–Rényi and Barabási–Albert random graphs

### Erdős–Rényi (ER) Graphs

In ER graphs, most nodes have a similar number of connections.

-   They are relatively resilient to the random removal of a few nodes because the connectivity is distributed relatively evenly. A cascade might be triggered if a critical density of nodes fails, overwhelming the capacity of the remaining network.

-   Since there are no specific "super-hubs," targeted attacks are not dramatically more effective than random failures unless a significant number of nodes are removed to fragment the network or the attack specifically targets a set of nodes that, if removed, would overload their collective neighbors beyond a critical point.

-   Once a cascade starts, it might spread more diffusely due to the homogeneity. However, if the overall network is stressed (e.g., low tolerance parameters in capacity models), even this diffuse spread can lead to widespread failure.

### Barabási–Albert (BA) Graphs

BA graphs are characterized by a few high-degree "hubs" (specific nodes that are highly connected) and many low-degree nodes.

-   Removing a random (likely low-degree) node typically has little impact on the overall network integrity because the hubs maintain connectivity.

-   However, they are more vulnerable to the targeted failure of their hubs. Since hubs are connected to a large number of other nodes and often handle significant portions of network flow or influence, their failure can: 
  1. Disconnect large parts of the network.
  2. Cause a massive redistribution of load onto their neighbors, which are often less capable of handling such an increase, thereby triggering a cascade. 
  
-   Contrary to ER networks, cascades in BA networks can be rapid and devastating. The heterogeneity of node loads means that some nodes are inherently much more critical than others. There can be a large range of tolerance parameters where scale-free networks are stable against random removals but unstable with respect to load-based removals of the highest-load nodes.


<!-- -   Do you expect Erdős–Rényi and Barabási–Albert random graphs to exhibit different properties in their susceptibility to cascading failures? Why? -->

<!-- -   For Barabási–Albert random graphs, do you expect the property of assortivity to come into play? -->

<!--     -   The fact is, when real-world networks develop, they build in structural stability to be resilient to failure. Assortivity can be a tool for this. Explain. -->

#### Assortativeness in Barabási–Albert Graphs

> "A network displays assortative mixing if the nodes in the networks that have many connections tend to be connected to other nodes with many connections. A network is said to be disassortative if the highly connected nodes tend to be connected to nodes with few connections." [@ash2007pasmaia, p.6]

Recall the "rich get richer" property of BA graphs. That means that high-degree nodes tend to connect to many low-degree nodes. That means the BA graph is generally resilient up to the point that the hub fails.


The failure of a hub immediately affects a large number of low-degree nodes, potentially isolating them or overloading them if they were dependent on it. Cascades might spread outwards from a failed hub to its many low-degree neighbors.

## Real-world networks' stability

Instead of purely random or preferential attachment, real-world networks can be grown with resilience in mind. This might involve selectively adding redundant links, reinforcing nodes that are becoming too critical, or ensuring that new nodes connect in ways that don't overly stress existing components. Assortativity can be a tool for this:

- By having important nodes (hubs) connect to each other, a robust core can be formed. If one hub in this core fails, other hubs within the core might be able to take up the slack or maintain critical pathways. This can create a resilient core.

- In naturally disassortative networks, recognizing that hubs connect to many low-degree, dependent nodes allows for targeted protection strategies.

- This structure can also help in "islanding" or modularizing failures. If the core is highly interconnected but has sparser connections to the periphery, a failure within the core might be devastating to the core itself but could potentially be prevented from spreading extensively to the rest of the network if the inter-modular connections are managed or severed. A failure within one module can be contained, preventing a global cascade.

- High clustering provides alternative pathways through which flows can pass, avoiding the failed component.

- Longer path lengths also promote robustness of the network. A disturbance needs to pass through a greater number of intermediate steps before the entire network is exposed to failure. Longer delays increase the change the network will resolve the perturbation.

#### The brain as an example of a Disassortative Network

- Certain control systems (hubs) in the brain might manage specific functions (loners).

- If such a control system (hub) fails, the downstream dependent functions (loners) will fail. However, this failure might be contained within that "module" without causing catastrophic failure of the entire system (e.g., damage to the occipital lobe causes loss of sight, not death).

- This disassortative structure can be a form of modular redundancy, preventing cascades from spreading throughout the entire system.

# Coding load-based models

- **Nodes**, $j = 1, 2, 3, \ldots N$, have an initial load, $L_j$, and capacity, $C_j$.

- The **initial load** is calculated based on node degree, $k_j$, and adjustable parameters $\alpha$ and $\beta$, that control the intensity of the initial load on nodes: $L_j = \beta k^{\alpha}_j$.

- The **capacity** $C_j$ of each node is proportional to its initial load and a tolerance factor, $T$: $C_j = L_j \cdot T$. If $T = 1.2$, nodes can handle 20% more than their initial load.

- Node $j$ will **fail** when the load it receives plus the initial load on its own node is greater than its processing capacity, $L_j + L_{ji} > C_j$

- When a node fails, its current load is **redistributed** among its still active neighbors. The redistribution is preferential: neighbors with higher degrees (or a function of degree related to $\alpha$) receive a larger fraction of the failed node's load: $\Delta L_{ji} = L_i \frac{k_j^{\alpha}}{\underset{n \in r_i}{\sum}k_n^{\alpha}}$.


```{r}
#| message: false
library(ggplot2)
library(tidyr)
library(dplyr)
library(igraph)
```

```{r xiang-function}
### "Complex" Xiang Cascade 

cascade.Xiang = function(g, beta = 1, T = 1.2, alpha = 1, max.steps = 10000) {
  N = vcount(g)
  deg = igraph::degree(g)
  # Load is based on beta, alpha, and degree
  load0 = beta * (deg^alpha)
  cap = T * load0
  # Choose high, average, and lowest degree vertices
  high = which.max(deg)
  low = which.min(deg)
  avg = which.min(abs(deg - mean(deg)))[1]
  seeds = list(High=high, Average=avg, Low=low)
  results = lapply(seeds, function(seed) {
    load = load0
    failed = logical(N)
    # Kill the target
    # Find neighbors
    # Distribute failure to other neighbors
    # High degree neighbors take more load
    failed[seed] = TRUE
    nbrs = as.integer(neighbors(g,seed))
    if (length(nbrs)) {
      wts = deg[nbrs]^alpha
      load[nbrs] = load[nbrs] + load[seed]*(wts/sum(wts))
    }
    load[seed] = 0
    
    series = sum(failed)/N
    t = 1
    while (t < max.steps) {
      to_fail = which(!failed & (load > cap))
      if (!length(to_fail)) break
      for (v in to_fail) {
        failed[v] = TRUE
        nbrs2 = setdiff(as.integer(neighbors(g,v)), which(failed))
        if (length(nbrs2)) {
          w2 = deg[nbrs2]^alpha
          load[nbrs2] = load[nbrs2] + load[v]*(w2/sum(w2))
        }
        load[v] = 0
      }
      t = t+1
      series[t] = sum(failed)/N
    }
    series
  })
  
  names(results) = names(seeds)
  results
}
```


```{r sim-funk}
sim.funk = function(rep_id, n_nodes) {
  set.seed(15846 + rep_id) 
  g_ba = sample_pa(n = n_nodes, power = 1, m = 2, directed = FALSE)
  g_er = sample_gnm(n = n_nodes, m = ecount(g_ba), directed = FALSE)
  g_sw = sample_smallworld(dim = 1, size = n_nodes, nei = floor(ecount(g_ba)/n_nodes), p = 0.05)
  ER.set = cascade.Xiang(g_er, beta = 1, T = 1.2, alpha = 1, max.steps = 1000)
  BA.set = cascade.Xiang(g_ba, beta = 1, T = 1.2, alpha = 1, max.steps = 1000)
  SW.set = cascade.Xiang(g_sw, beta = 1, T = 1.2, alpha = 1, max.steps = 1000)
  list(
    ER_high = ER.set$High,
    ER_avg  = ER.set$Average,
    ER_low  = ER.set$Low,
    BA_high = BA.set$High,
    BA_avg  = BA.set$Average,
    BA_low  = BA.set$Low,
    SW_high = SW.set$High,
    SW_avg  = SW.set$Average,
    SW_low  = SW.set$Low
  )
}
```

```{r}
n_nodes = 30
results = sim.funk(34, n_nodes = n_nodes)
models = c("ER","BA","SW")
max_len = 30
op = par(mfrow=c(3,1), mar=c(4,4,2,1))

for(m in models) {
  idx = grep(paste0("^", m, "_"), names(results))
  raw = results[idx]
  mat = sapply(raw, function(v) {
    L = length(v)
    if (L < max_len) {
      c(v, rep(v[L], max_len - L))
    } else {
      v[1:max_len]
    }
  })
  
  matplot(1:max_len, mat,
          type = "o", pch = 1:3,
          col = 1:3, xlab = "Step",
          ylab = "Prop. Failed",
          main = paste(m, "model"),
          xlim = c(1, max_len),
          ylim = c(0, 1))
  legend("bottomright", legend = names(results)[idx],
         col = 1:3, pch = 1:3, bty = "n")
}
par(op)
```

### What do we see?

- **ER & SW**: Tend to show more uniform failure patterns. The specific starting node (high, avg, low degree) has less differential impact because connectivity is more homogeneous. SW models might take longer to fail completely due to their lattice-like local structure.

- **BA**: Show significant heterogeneity. Failing a high-degree hub often leads to rapid and extensive cascades, potentially total failure. Failing a low-degree node might have minimal impact. This is because hubs are often the sole connection for many other nodes.

The choice of which "central" node to target for intervention or to study for failure initiation only makes sense after you understand the network's underlying degree distribution (topology). Targeting a high-degree node is critical in a power-law network but less so in an ER network.



# Discussion

- For Psychological Networks, how do abstract concepts like "load," "capacity," and "threshold" translate?

- In a electrical power grid, repairs of damaged equipment, frequent maintenance, equipment replacement with more potent ones, or using alarm systems, are some response options to reduce the probability of blackouts. In psychological networks, 
  - What could be an example of a weak system? 
  - What could be mitigation and reinforcement strategies to prevent or contain cascades? How to identify and reinforce critical components?


# Disclaimer

MC used AI to help with grammar and to generate ideas.

# References
